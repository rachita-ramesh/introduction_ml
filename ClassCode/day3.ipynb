{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### arrays, one d, 2 d arrays (matrices vectors)\n",
    "#### tensors ##### accelerated over a gpu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1,2,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu') #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1,2,2,3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Gradient descent (gradient matrix) \n",
    "torch.zeros([1,2,3],requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minmize $X^2+4X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x ### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Grad of x at 0 wrt z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x*x+4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad ### Start accumulating the value of gradients,\n",
    "### Pass 1\n",
    "### grad=4\n",
    "### Pass 2\n",
    "### grad_now+grad_prev=4+4=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward pass\n",
    "#### backward()\n",
    "#### zero out the graidients\n",
    "### xnew=xold-lr*(sum(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z 0.0, x: -0.03999999910593033\n",
      "Z -0.15839999914169312, x: -0.07919999957084656\n",
      "Z -0.31052735447883606, x: -0.11761599779129028\n",
      "Z -0.4566304683685303, x: -0.15526367723941803\n",
      "Z -0.5969479084014893, x: -0.19215840101242065\n",
      "Z -0.7317087650299072, x: -0.22831523418426514\n",
      "Z -0.8611330986022949, x: -0.2637489140033722\n",
      "Z -0.9854321479797363, x: -0.29847392439842224\n",
      "Z -1.104809045791626, x: -0.3325044512748718\n",
      "Z -1.2194585800170898, x: -0.3658543527126312\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(0.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(10):\n",
    "    z=x*x+4*x\n",
    "    z.backward() ### dz/dx\n",
    "    with torch.no_grad(): ##Disables any gradient computation\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Autodiff\n",
    "### xy=750=>x=750/y\n",
    "## x+10y Minimize this\n",
    "### 750/y+y*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: 760.0, x: 8.399999618530273\n",
      "Z: 173.2857208251953, x: 8.406291961669922\n",
      "Z: 173.28179931640625, x: 8.41242504119873\n",
      "Z: 173.27809143066406, x: 8.418403625488281\n",
      "Z: 173.27456665039062, x: 8.42423152923584\n",
      "Z: 173.27120971679688, x: 8.429913520812988\n",
      "Z: 173.2680206298828, x: 8.435453414916992\n",
      "Z: 173.26498413085938, x: 8.4408540725708\n",
      "Z: 173.26211547851562, x: 8.446120262145996\n",
      "Z: 173.25936889648438, x: 8.451254844665527\n",
      "Z: 173.25677490234375, x: 8.45626163482666\n",
      "Z: 173.25428771972656, x: 8.46114444732666\n",
      "Z: 173.25193786621094, x: 8.465906143188477\n",
      "Z: 173.24969482421875, x: 8.470550537109375\n",
      "Z: 173.24755859375, x: 8.475079536437988\n",
      "Z: 173.24554443359375, x: 8.479496955871582\n",
      "Z: 173.2436065673828, x: 8.483805656433105\n",
      "Z: 173.2417755126953, x: 8.488008499145508\n",
      "Z: 173.24002075195312, x: 8.492108345031738\n",
      "Z: 173.23837280273438, x: 8.496108055114746\n",
      "Z: 173.23678588867188, x: 8.500009536743164\n",
      "Z: 173.23529052734375, x: 8.503815650939941\n",
      "Z: 173.23385620117188, x: 8.507528305053711\n",
      "Z: 173.2324981689453, x: 8.511151313781738\n",
      "Z: 173.231201171875, x: 8.51468563079834\n",
      "Z: 173.22998046875, x: 8.518134117126465\n",
      "Z: 173.22879028320312, x: 8.521498680114746\n",
      "Z: 173.22767639160156, x: 8.524782180786133\n",
      "Z: 173.22659301757812, x: 8.527985572814941\n",
      "Z: 173.2255859375, x: 8.531111717224121\n",
      "Z: 173.22463989257812, x: 8.534162521362305\n",
      "Z: 173.22372436523438, x: 8.537138938903809\n",
      "Z: 173.22283935546875, x: 8.540043830871582\n",
      "Z: 173.2220001220703, x: 8.542879104614258\n",
      "Z: 173.22120666503906, x: 8.545645713806152\n",
      "Z: 173.220458984375, x: 8.548345565795898\n",
      "Z: 173.2197265625, x: 8.550980567932129\n",
      "Z: 173.21905517578125, x: 8.553552627563477\n",
      "Z: 173.2183837890625, x: 8.556062698364258\n",
      "Z: 173.2177734375, x: 8.558512687683105\n",
      "Z: 173.21717834472656, x: 8.560904502868652\n",
      "Z: 173.21661376953125, x: 8.563239097595215\n",
      "Z: 173.216064453125, x: 8.56551742553711\n",
      "Z: 173.21556091308594, x: 8.567741394042969\n",
      "Z: 173.21507263183594, x: 8.569912910461426\n",
      "Z: 173.214599609375, x: 8.57203197479248\n",
      "Z: 173.21417236328125, x: 8.574100494384766\n",
      "Z: 173.2137451171875, x: 8.576120376586914\n",
      "Z: 173.21331787109375, x: 8.578091621398926\n",
      "Z: 173.21295166015625, x: 8.580016136169434\n",
      "Z: 173.21258544921875, x: 8.581894874572754\n",
      "Z: 173.2122344970703, x: 8.58372974395752\n",
      "Z: 173.2119140625, x: 8.58552074432373\n",
      "Z: 173.21157836914062, x: 8.587268829345703\n",
      "Z: 173.21127319335938, x: 8.58897590637207\n",
      "Z: 173.21099853515625, x: 8.590642929077148\n",
      "Z: 173.21072387695312, x: 8.592269897460938\n",
      "Z: 173.21044921875, x: 8.59385871887207\n",
      "Z: 173.210205078125, x: 8.595409393310547\n",
      "Z: 173.20997619628906, x: 8.596923828125\n",
      "Z: 173.20974731445312, x: 8.598402976989746\n",
      "Z: 173.20953369140625, x: 8.599846839904785\n",
      "Z: 173.20932006835938, x: 8.601256370544434\n",
      "Z: 173.20913696289062, x: 8.602632522583008\n",
      "Z: 173.2089385986328, x: 8.603976249694824\n",
      "Z: 173.20877075195312, x: 8.6052885055542\n",
      "Z: 173.20858764648438, x: 8.60657024383545\n",
      "Z: 173.20843505859375, x: 8.607821464538574\n",
      "Z: 173.20828247070312, x: 8.60904312133789\n",
      "Z: 173.2081298828125, x: 8.610236167907715\n",
      "Z: 173.20799255371094, x: 8.611401557922363\n",
      "Z: 173.20785522460938, x: 8.612539291381836\n",
      "Z: 173.20773315429688, x: 8.61365032196045\n",
      "Z: 173.20761108398438, x: 8.61473560333252\n",
      "Z: 173.20748901367188, x: 8.615795135498047\n",
      "Z: 173.20736694335938, x: 8.616829872131348\n",
      "Z: 173.207275390625, x: 8.617840766906738\n",
      "Z: 173.2071533203125, x: 8.618827819824219\n",
      "Z: 173.2070770263672, x: 8.619791030883789\n",
      "Z: 173.20697021484375, x: 8.620732307434082\n",
      "Z: 173.20687866210938, x: 8.621651649475098\n",
      "Z: 173.20680236816406, x: 8.622549057006836\n",
      "Z: 173.20672607421875, x: 8.623425483703613\n",
      "Z: 173.20664978027344, x: 8.624281883239746\n",
      "Z: 173.20657348632812, x: 8.625118255615234\n",
      "Z: 173.20651245117188, x: 8.625934600830078\n",
      "Z: 173.20645141601562, x: 8.626731872558594\n",
      "Z: 173.20639038085938, x: 8.627511024475098\n",
      "Z: 173.20632934570312, x: 8.628271102905273\n",
      "Z: 173.20626831054688, x: 8.629014015197754\n",
      "Z: 173.20620727539062, x: 8.629739761352539\n",
      "Z: 173.20614624023438, x: 8.630448341369629\n",
      "Z: 173.20611572265625, x: 8.63114070892334\n",
      "Z: 173.2060546875, x: 8.631816864013672\n",
      "Z: 173.20602416992188, x: 8.632476806640625\n",
      "Z: 173.2059783935547, x: 8.633121490478516\n",
      "Z: 173.2059326171875, x: 8.633750915527344\n",
      "Z: 173.20590209960938, x: 8.634366035461426\n",
      "Z: 173.2058563232422, x: 8.634966850280762\n",
      "Z: 173.20582580566406, x: 8.635553359985352\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=(750/x)+x*10\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x-=lr*x.grad\n",
    "        x.grad.zero_()\n",
    "    print(f\"Z: {z}, x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ML problem: \n",
    "import pandas as pd\n",
    "reg=pd.read_csv(\"../sony/data/regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1  15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "\n",
       "   origin  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mpg=b0+b1*cyl ## as a matrix product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dloss/db0, dloss/db1\n",
    "#### loss=f(b0,b1)\n",
    "##loss=eq\n",
    "##loss.backward()\n",
    "##b0.grad\n",
    "##b1.grad\n",
    "X=reg[['cylinders']].values\n",
    "y=reg[['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## W dim?=> dloss/dW,dloss/db\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss?\n",
    "loss=sum((y-XW+b)^2)/n\n",
    "loss.backward()\n",
    "W.grad\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 106.57937383717696, W: [[1.5394461]], b: [-12.498031]\n",
      "Loss: 106.29652724282161, W: [[1.5306689]], b: [-12.550415]\n",
      "Loss: 106.01465406982496, W: [[1.5219065]], b: [-12.602709]\n",
      "Loss: 105.73375239029292, W: [[1.5131594]], b: [-12.654913]\n",
      "Loss: 105.4538148533757, W: [[1.5044272]], b: [-12.707027]\n",
      "Loss: 105.17483884401985, W: [[1.4957101]], b: [-12.759052]\n",
      "Loss: 104.8968210069427, W: [[1.4870082]], b: [-12.810987]\n",
      "Loss: 104.61976103636141, W: [[1.4783207]], b: [-12.862833]\n",
      "Loss: 104.34365403355959, W: [[1.4696487]], b: [-12.91459]\n",
      "Loss: 104.06849371735905, W: [[1.4609914]], b: [-12.966257]\n",
      "Loss: 103.79428275126844, W: [[1.452349]], b: [-13.017836]\n",
      "Loss: 103.52101361310103, W: [[1.4437215]], b: [-13.069325]\n",
      "Loss: 103.24868271208203, W: [[1.4351087]], b: [-13.120727]\n",
      "Loss: 102.97728938506957, W: [[1.426511]], b: [-13.17204]\n",
      "Loss: 102.70682585822372, W: [[1.417928]], b: [-13.223265]\n",
      "Loss: 102.43729364242495, W: [[1.4093596]], b: [-13.274402]\n",
      "Loss: 102.16868574302856, W: [[1.4008062]], b: [-13.32545]\n",
      "Loss: 101.9010047640723, W: [[1.3922671]], b: [-13.3764105]\n",
      "Loss: 101.63424392719688, W: [[1.383743]], b: [-13.427283]\n",
      "Loss: 101.36840038466318, W: [[1.3752334]], b: [-13.478069]\n",
      "Loss: 101.10346696857538, W: [[1.3667387]], b: [-13.528768]\n",
      "Loss: 100.83944657207539, W: [[1.3582585]], b: [-13.579379]\n",
      "Loss: 100.57633189425486, W: [[1.3497927]], b: [-13.629903]\n",
      "Loss: 100.31412440868142, W: [[1.3413416]], b: [-13.68034]\n",
      "Loss: 100.05281752603965, W: [[1.332905]], b: [-13.73069]\n",
      "Loss: 99.79240902915474, W: [[1.3244832]], b: [-13.780953]\n",
      "Loss: 99.53289683086567, W: [[1.3160756]], b: [-13.831131]\n",
      "Loss: 99.27427338304174, W: [[1.3076826]], b: [-13.881222]\n",
      "Loss: 99.01654162565809, W: [[1.2993039]], b: [-13.931227]\n",
      "Loss: 98.75969341752857, W: [[1.2909396]], b: [-13.981146]\n",
      "Loss: 98.5037277684382, W: [[1.2825896]], b: [-14.030979]\n",
      "Loss: 98.24864168729665, W: [[1.2742538]], b: [-14.080727]\n",
      "Loss: 97.99443261730507, W: [[1.2659328]], b: [-14.130388]\n",
      "Loss: 97.74109908437582, W: [[1.257626]], b: [-14.179964]\n",
      "Loss: 97.48863842243667, W: [[1.2493335]], b: [-14.229455]\n",
      "Loss: 97.23704367063091, W: [[1.2410551]], b: [-14.278861]\n",
      "Loss: 96.98631226261628, W: [[1.2327911]], b: [-14.328182]\n",
      "Loss: 96.73644225327435, W: [[1.2245412]], b: [-14.3774185]\n",
      "Loss: 96.48743114163213, W: [[1.2163055]], b: [-14.42657]\n",
      "Loss: 96.23927704862936, W: [[1.208084]], b: [-14.4756365]\n",
      "Loss: 95.99197747220049, W: [[1.1998769]], b: [-14.524619]\n",
      "Loss: 95.74552638659085, W: [[1.1916834]], b: [-14.573517]\n",
      "Loss: 95.49992469960344, W: [[1.1835042]], b: [-14.622331]\n",
      "Loss: 95.25516651145746, W: [[1.175339]], b: [-14.671061]\n",
      "Loss: 95.01124919913075, W: [[1.1671882]], b: [-14.719707]\n",
      "Loss: 94.76817127068546, W: [[1.1590513]], b: [-14.768269]\n",
      "Loss: 94.5259298817889, W: [[1.1509284]], b: [-14.816748]\n",
      "Loss: 94.28451859372065, W: [[1.1428194]], b: [-14.865143]\n",
      "Loss: 94.04393987965155, W: [[1.1347246]], b: [-14.913455]\n",
      "Loss: 93.80418681530303, W: [[1.1266433]], b: [-14.961684]\n",
      "Loss: 93.56525705880594, W: [[1.118576]], b: [-15.00983]\n",
      "Loss: 93.32714842416925, W: [[1.1105225]], b: [-15.057894]\n",
      "Loss: 93.08985907273593, W: [[1.102483]], b: [-15.105874]\n",
      "Loss: 92.85338694636683, W: [[1.0944574]], b: [-15.153772]\n",
      "Loss: 92.6177251570165, W: [[1.0864456]], b: [-15.201588]\n",
      "Loss: 92.38287626920909, W: [[1.0784473]], b: [-15.249321]\n",
      "Loss: 92.14883341928909, W: [[1.0704631]], b: [-15.296972]\n",
      "Loss: 91.9155945857501, W: [[1.0624925]], b: [-15.344542]\n",
      "Loss: 91.68315825547626, W: [[1.0545356]], b: [-15.392029]\n",
      "Loss: 91.45152124433464, W: [[1.0465924]], b: [-15.439435]\n",
      "Loss: 91.22067785915773, W: [[1.038663]], b: [-15.486759]\n",
      "Loss: 90.99063023209378, W: [[1.030747]], b: [-15.534002]\n",
      "Loss: 90.76137154011904, W: [[1.0228447]], b: [-15.581163]\n",
      "Loss: 90.53290406672578, W: [[1.0149562]], b: [-15.628243]\n",
      "Loss: 90.30522244473264, W: [[1.0070812]], b: [-15.675243]\n",
      "Loss: 90.07831923122174, W: [[0.99921966]], b: [-15.722162]\n",
      "Loss: 89.85219662331951, W: [[0.99137163]], b: [-15.769]\n",
      "Loss: 89.62685315078838, W: [[0.983537]], b: [-15.815758]\n",
      "Loss: 89.40228213898385, W: [[0.9757159]], b: [-15.862434]\n",
      "Loss: 89.17848586582821, W: [[0.96790826]], b: [-15.909031]\n",
      "Loss: 88.95545795402309, W: [[0.9601142]], b: [-15.955547]\n",
      "Loss: 88.73319685791502, W: [[0.95233357]], b: [-16.001984]\n",
      "Loss: 88.51170043897277, W: [[0.9445662]], b: [-16.04834]\n",
      "Loss: 88.29096583434749, W: [[0.9368121]], b: [-16.094616]\n",
      "Loss: 88.07099186830436, W: [[0.9290715]], b: [-16.140814]\n",
      "Loss: 87.85176787295342, W: [[0.9213443]], b: [-16.186932]\n",
      "Loss: 87.63330025743231, W: [[0.9136301]], b: [-16.23297]\n",
      "Loss: 87.41558726129708, W: [[0.90592915]], b: [-16.278929]\n",
      "Loss: 87.1986177239991, W: [[0.8982418]], b: [-16.324808]\n",
      "Loss: 86.98239927493378, W: [[0.89056754]], b: [-16.37061]\n",
      "Loss: 86.7669205094124, W: [[0.88290644]], b: [-16.416332]\n",
      "Loss: 86.55217989465888, W: [[0.8752587]], b: [-16.461975]\n",
      "Loss: 86.33818422257363, W: [[0.86762387]], b: [-16.50754]\n",
      "Loss: 86.12492258030417, W: [[0.8600023]], b: [-16.553026]\n",
      "Loss: 85.91239314884028, W: [[0.8523936]], b: [-16.598434]\n",
      "Loss: 85.70059406368182, W: [[0.84479827]], b: [-16.643764]\n",
      "Loss: 85.48952348366416, W: [[0.83721596]], b: [-16.689016]\n",
      "Loss: 85.27917971123543, W: [[0.8296469]], b: [-16.734192]\n",
      "Loss: 85.06955201714128, W: [[0.82209045]], b: [-16.77929]\n",
      "Loss: 84.860646820288, W: [[0.81454694]], b: [-16.824308]\n",
      "Loss: 84.65246222574288, W: [[0.80701655]], b: [-16.869251]\n",
      "Loss: 84.44498858129145, W: [[0.7994992]], b: [-16.914116]\n",
      "Loss: 84.23823210859764, W: [[0.7919949]], b: [-16.958902]\n",
      "Loss: 84.03219094634537, W: [[0.7845033]], b: [-17.003613]\n",
      "Loss: 83.82685486704993, W: [[0.7770247]], b: [-17.048246]\n",
      "Loss: 83.62222155257871, W: [[0.7695587]], b: [-17.092804]\n",
      "Loss: 83.41828970460764, W: [[0.76210576]], b: [-17.137283]\n",
      "Loss: 83.21506570844782, W: [[0.75466555]], b: [-17.181686]\n",
      "Loss: 83.01253908716674, W: [[0.7472385]], b: [-17.226013]\n",
      "Loss: 82.81070862636678, W: [[0.73982394]], b: [-17.270264]\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(100):\n",
    "    diff=y-torch.matmul(X.float(),W)+b\n",
    "    loss=sum(diff*diff)/y.shape[0]\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W.detach().numpy()}, b: {b.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### linear classifier.\n",
    "### Can you estimate a linear classifier using autodiff\n",
    "### Loss for linear classifier?\n",
    "### log loss as a function of W and b, p=f(X,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=pd.read_csv(\"../sony/data/classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_pregnant</th>\n",
       "      <th>Plasma_glucose</th>\n",
       "      <th>Blood_pres</th>\n",
       "      <th>Skin_thick</th>\n",
       "      <th>Serum_insu</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Diabetes_func</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No_pregnant  Plasma_glucose  Blood_pres  Skin_thick  Serum_insu   BMI  \\\n",
       "0            6             148          72          35           0  33.6   \n",
       "1            1              85          66          29           0  26.6   \n",
       "\n",
       "   Diabetes_func  Age  Class  \n",
       "0          0.627   50      1  \n",
       "1          0.351   31      0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cls[['No_pregnant']].values\n",
    "y=cls[['Class']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss=‚àí[ùë¶ùëôùëúùëî(ùëù+tol)+(1‚àíùë¶)ùëôùëúùëî(1‚àíùëù+tol)]\n",
    "### p=1/(1+e^-z)\n",
    "### z= XW+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5830082893371582, W: tensor([[0.2285]], requires_grad=True), b: tensor([1.4390], requires_grad=True)\n",
      "Loss: 1.5427392721176147, W: tensor([[0.2093]], requires_grad=True), b: tensor([1.4335], requires_grad=True)\n",
      "Loss: 1.5032926797866821, W: tensor([[0.1904]], requires_grad=True), b: tensor([1.4282], requires_grad=True)\n",
      "Loss: 1.4647493362426758, W: tensor([[0.1717]], requires_grad=True), b: tensor([1.4228], requires_grad=True)\n",
      "Loss: 1.4271955490112305, W: tensor([[0.1533]], requires_grad=True), b: tensor([1.4175], requires_grad=True)\n",
      "Loss: 1.3907232284545898, W: tensor([[0.1351]], requires_grad=True), b: tensor([1.4123], requires_grad=True)\n",
      "Loss: 1.3554285764694214, W: tensor([[0.1173]], requires_grad=True), b: tensor([1.4071], requires_grad=True)\n",
      "Loss: 1.321410059928894, W: tensor([[0.0999]], requires_grad=True), b: tensor([1.4020], requires_grad=True)\n",
      "Loss: 1.288766622543335, W: tensor([[0.0829]], requires_grad=True), b: tensor([1.3970], requires_grad=True)\n",
      "Loss: 1.2575944662094116, W: tensor([[0.0663]], requires_grad=True), b: tensor([1.3921], requires_grad=True)\n",
      "Loss: 1.22798490524292, W: tensor([[0.0501]], requires_grad=True), b: tensor([1.3872], requires_grad=True)\n",
      "Loss: 1.200018286705017, W: tensor([[0.0345]], requires_grad=True), b: tensor([1.3824], requires_grad=True)\n",
      "Loss: 1.1737607717514038, W: tensor([[0.0194]], requires_grad=True), b: tensor([1.3777], requires_grad=True)\n",
      "Loss: 1.1492608785629272, W: tensor([[0.0049]], requires_grad=True), b: tensor([1.3731], requires_grad=True)\n",
      "Loss: 1.1265448331832886, W: tensor([[-0.0090]], requires_grad=True), b: tensor([1.3686], requires_grad=True)\n",
      "Loss: 1.1056150197982788, W: tensor([[-0.0223]], requires_grad=True), b: tensor([1.3641], requires_grad=True)\n",
      "Loss: 1.0864485502243042, W: tensor([[-0.0349]], requires_grad=True), b: tensor([1.3598], requires_grad=True)\n",
      "Loss: 1.0689982175827026, W: tensor([[-0.0469]], requires_grad=True), b: tensor([1.3556], requires_grad=True)\n",
      "Loss: 1.0531946420669556, W: tensor([[-0.0583]], requires_grad=True), b: tensor([1.3514], requires_grad=True)\n",
      "Loss: 1.0389496088027954, W: tensor([[-0.0690]], requires_grad=True), b: tensor([1.3474], requires_grad=True)\n",
      "Loss: 1.0261621475219727, W: tensor([[-0.0791]], requires_grad=True), b: tensor([1.3434], requires_grad=True)\n",
      "Loss: 1.0147205591201782, W: tensor([[-0.0886]], requires_grad=True), b: tensor([1.3396], requires_grad=True)\n",
      "Loss: 1.0045104026794434, W: tensor([[-0.0975]], requires_grad=True), b: tensor([1.3358], requires_grad=True)\n",
      "Loss: 0.9954161047935486, W: tensor([[-0.1058]], requires_grad=True), b: tensor([1.3321], requires_grad=True)\n",
      "Loss: 0.987325131893158, W: tensor([[-0.1136]], requires_grad=True), b: tensor([1.3285], requires_grad=True)\n",
      "Loss: 0.9801309704780579, W: tensor([[-0.1209]], requires_grad=True), b: tensor([1.3249], requires_grad=True)\n",
      "Loss: 0.9737336039543152, W: tensor([[-0.1278]], requires_grad=True), b: tensor([1.3214], requires_grad=True)\n",
      "Loss: 0.9680419564247131, W: tensor([[-0.1341]], requires_grad=True), b: tensor([1.3180], requires_grad=True)\n",
      "Loss: 0.9629726409912109, W: tensor([[-0.1401]], requires_grad=True), b: tensor([1.3147], requires_grad=True)\n",
      "Loss: 0.9584508538246155, W: tensor([[-0.1456]], requires_grad=True), b: tensor([1.3114], requires_grad=True)\n",
      "Loss: 0.9544102549552917, W: tensor([[-0.1508]], requires_grad=True), b: tensor([1.3082], requires_grad=True)\n",
      "Loss: 0.9507915377616882, W: tensor([[-0.1556]], requires_grad=True), b: tensor([1.3050], requires_grad=True)\n",
      "Loss: 0.9475427269935608, W: tensor([[-0.1601]], requires_grad=True), b: tensor([1.3019], requires_grad=True)\n",
      "Loss: 0.9446180462837219, W: tensor([[-0.1643]], requires_grad=True), b: tensor([1.2988], requires_grad=True)\n",
      "Loss: 0.9419770240783691, W: tensor([[-0.1682]], requires_grad=True), b: tensor([1.2957], requires_grad=True)\n",
      "Loss: 0.9395846724510193, W: tensor([[-0.1718]], requires_grad=True), b: tensor([1.2927], requires_grad=True)\n",
      "Loss: 0.9374103546142578, W: tensor([[-0.1752]], requires_grad=True), b: tensor([1.2898], requires_grad=True)\n",
      "Loss: 0.9354269504547119, W: tensor([[-0.1784]], requires_grad=True), b: tensor([1.2869], requires_grad=True)\n",
      "Loss: 0.9336112141609192, W: tensor([[-0.1813]], requires_grad=True), b: tensor([1.2840], requires_grad=True)\n",
      "Loss: 0.9319422841072083, W: tensor([[-0.1841]], requires_grad=True), b: tensor([1.2811], requires_grad=True)\n",
      "Loss: 0.9304027557373047, W: tensor([[-0.1866]], requires_grad=True), b: tensor([1.2783], requires_grad=True)\n",
      "Loss: 0.9289767742156982, W: tensor([[-0.1890]], requires_grad=True), b: tensor([1.2755], requires_grad=True)\n",
      "Loss: 0.9276506304740906, W: tensor([[-0.1912]], requires_grad=True), b: tensor([1.2727], requires_grad=True)\n",
      "Loss: 0.9264126420021057, W: tensor([[-0.1933]], requires_grad=True), b: tensor([1.2700], requires_grad=True)\n",
      "Loss: 0.9252517819404602, W: tensor([[-0.1952]], requires_grad=True), b: tensor([1.2672], requires_grad=True)\n",
      "Loss: 0.9241597056388855, W: tensor([[-0.1970]], requires_grad=True), b: tensor([1.2645], requires_grad=True)\n",
      "Loss: 0.9231278300285339, W: tensor([[-0.1987]], requires_grad=True), b: tensor([1.2619], requires_grad=True)\n",
      "Loss: 0.9221492409706116, W: tensor([[-0.2002]], requires_grad=True), b: tensor([1.2592], requires_grad=True)\n",
      "Loss: 0.9212180972099304, W: tensor([[-0.2016]], requires_grad=True), b: tensor([1.2566], requires_grad=True)\n",
      "Loss: 0.9203285574913025, W: tensor([[-0.2029]], requires_grad=True), b: tensor([1.2540], requires_grad=True)\n",
      "Loss: 0.9194760918617249, W: tensor([[-0.2042]], requires_grad=True), b: tensor([1.2513], requires_grad=True)\n",
      "Loss: 0.9186564087867737, W: tensor([[-0.2053]], requires_grad=True), b: tensor([1.2488], requires_grad=True)\n",
      "Loss: 0.9178659319877625, W: tensor([[-0.2063]], requires_grad=True), b: tensor([1.2462], requires_grad=True)\n",
      "Loss: 0.9171013236045837, W: tensor([[-0.2073]], requires_grad=True), b: tensor([1.2436], requires_grad=True)\n",
      "Loss: 0.9163598418235779, W: tensor([[-0.2082]], requires_grad=True), b: tensor([1.2411], requires_grad=True)\n",
      "Loss: 0.9156387448310852, W: tensor([[-0.2090]], requires_grad=True), b: tensor([1.2386], requires_grad=True)\n",
      "Loss: 0.9149360656738281, W: tensor([[-0.2097]], requires_grad=True), b: tensor([1.2360], requires_grad=True)\n",
      "Loss: 0.9142497181892395, W: tensor([[-0.2104]], requires_grad=True), b: tensor([1.2335], requires_grad=True)\n",
      "Loss: 0.9135779738426208, W: tensor([[-0.2111]], requires_grad=True), b: tensor([1.2310], requires_grad=True)\n",
      "Loss: 0.9129191040992737, W: tensor([[-0.2116]], requires_grad=True), b: tensor([1.2285], requires_grad=True)\n",
      "Loss: 0.9122722148895264, W: tensor([[-0.2121]], requires_grad=True), b: tensor([1.2261], requires_grad=True)\n",
      "Loss: 0.9116355776786804, W: tensor([[-0.2126]], requires_grad=True), b: tensor([1.2236], requires_grad=True)\n",
      "Loss: 0.9110085368156433, W: tensor([[-0.2130]], requires_grad=True), b: tensor([1.2212], requires_grad=True)\n",
      "Loss: 0.9103897213935852, W: tensor([[-0.2134]], requires_grad=True), b: tensor([1.2187], requires_grad=True)\n",
      "Loss: 0.9097787737846375, W: tensor([[-0.2137]], requires_grad=True), b: tensor([1.2163], requires_grad=True)\n",
      "Loss: 0.9091746807098389, W: tensor([[-0.2140]], requires_grad=True), b: tensor([1.2138], requires_grad=True)\n",
      "Loss: 0.9085767865180969, W: tensor([[-0.2143]], requires_grad=True), b: tensor([1.2114], requires_grad=True)\n",
      "Loss: 0.9079844951629639, W: tensor([[-0.2145]], requires_grad=True), b: tensor([1.2090], requires_grad=True)\n",
      "Loss: 0.9073973298072815, W: tensor([[-0.2147]], requires_grad=True), b: tensor([1.2066], requires_grad=True)\n",
      "Loss: 0.9068149924278259, W: tensor([[-0.2149]], requires_grad=True), b: tensor([1.2042], requires_grad=True)\n",
      "Loss: 0.906236469745636, W: tensor([[-0.2150]], requires_grad=True), b: tensor([1.2018], requires_grad=True)\n",
      "Loss: 0.9056623578071594, W: tensor([[-0.2151]], requires_grad=True), b: tensor([1.1994], requires_grad=True)\n",
      "Loss: 0.9050914645195007, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1970], requires_grad=True)\n",
      "Loss: 0.9045238494873047, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1946], requires_grad=True)\n",
      "Loss: 0.9039594531059265, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1923], requires_grad=True)\n",
      "Loss: 0.9033976197242737, W: tensor([[-0.2153]], requires_grad=True), b: tensor([1.1899], requires_grad=True)\n",
      "Loss: 0.9028384685516357, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1875], requires_grad=True)\n",
      "Loss: 0.9022815823554993, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1852], requires_grad=True)\n",
      "Loss: 0.901727020740509, W: tensor([[-0.2152]], requires_grad=True), b: tensor([1.1828], requires_grad=True)\n",
      "Loss: 0.9011746048927307, W: tensor([[-0.2151]], requires_grad=True), b: tensor([1.1805], requires_grad=True)\n",
      "Loss: 0.9006239771842957, W: tensor([[-0.2150]], requires_grad=True), b: tensor([1.1782], requires_grad=True)\n",
      "Loss: 0.9000754356384277, W: tensor([[-0.2149]], requires_grad=True), b: tensor([1.1758], requires_grad=True)\n",
      "Loss: 0.8995283246040344, W: tensor([[-0.2148]], requires_grad=True), b: tensor([1.1735], requires_grad=True)\n",
      "Loss: 0.8989830613136292, W: tensor([[-0.2147]], requires_grad=True), b: tensor([1.1712], requires_grad=True)\n",
      "Loss: 0.898439347743988, W: tensor([[-0.2145]], requires_grad=True), b: tensor([1.1688], requires_grad=True)\n",
      "Loss: 0.8978970646858215, W: tensor([[-0.2144]], requires_grad=True), b: tensor([1.1665], requires_grad=True)\n",
      "Loss: 0.8973562717437744, W: tensor([[-0.2142]], requires_grad=True), b: tensor([1.1642], requires_grad=True)\n",
      "Loss: 0.8968166708946228, W: tensor([[-0.2140]], requires_grad=True), b: tensor([1.1619], requires_grad=True)\n",
      "Loss: 0.8962785601615906, W: tensor([[-0.2139]], requires_grad=True), b: tensor([1.1596], requires_grad=True)\n",
      "Loss: 0.8957417011260986, W: tensor([[-0.2137]], requires_grad=True), b: tensor([1.1573], requires_grad=True)\n",
      "Loss: 0.8952059745788574, W: tensor([[-0.2135]], requires_grad=True), b: tensor([1.1550], requires_grad=True)\n",
      "Loss: 0.8946714997291565, W: tensor([[-0.2133]], requires_grad=True), b: tensor([1.1527], requires_grad=True)\n",
      "Loss: 0.8941381573677063, W: tensor([[-0.2130]], requires_grad=True), b: tensor([1.1504], requires_grad=True)\n",
      "Loss: 0.8936060070991516, W: tensor([[-0.2128]], requires_grad=True), b: tensor([1.1481], requires_grad=True)\n",
      "Loss: 0.8930748105049133, W: tensor([[-0.2126]], requires_grad=True), b: tensor([1.1458], requires_grad=True)\n",
      "Loss: 0.8925447463989258, W: tensor([[-0.2123]], requires_grad=True), b: tensor([1.1435], requires_grad=True)\n",
      "Loss: 0.8920158743858337, W: tensor([[-0.2121]], requires_grad=True), b: tensor([1.1412], requires_grad=True)\n",
      "Loss: 0.8914878964424133, W: tensor([[-0.2118]], requires_grad=True), b: tensor([1.1389], requires_grad=True)\n",
      "Loss: 0.8909609913825989, W: tensor([[-0.2116]], requires_grad=True), b: tensor([1.1366], requires_grad=True)\n",
      "Loss: 0.8904352188110352, W: tensor([[-0.2113]], requires_grad=True), b: tensor([1.1344], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)\n",
    "W=torch.randn(1,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)\n",
    "tol=0.0000000001\n",
    "lr=0.01\n",
    "for i in range(100):\n",
    "    z=torch.matmul(X.float(),W)+b\n",
    "    p=1.0/(1+torch.exp(-z))\n",
    "    loss=-(y*torch.log(p+tol)+(1-y)*torch.log(1-p+tol)).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W-=lr*W.grad\n",
    "        b-=lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}, W: {W}, b: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP, define models, batches of data very easily, optimizers, loss functions\n",
    "mnist=pd.read_csv(\"../sony/data/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=np.array([[0.8,0.1,0.1],[0.7,0.2,0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.1, 0.1],\n",
       "       [0.7, 0.2, 0.1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals=np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.1, 0.1],\n",
       "       [0.7, 0.2, 0.1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 0.2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[range(2),actuals] ## for all rows give me values in columns denoted by indices in actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=mnist['label'].values\n",
    "X=mnist.drop('label',axis=1).values\n",
    "X=X/255.0\n",
    "X=torch.tensor(X)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=torch.randn((784,3),requires_grad=True)\n",
    "b1=torch.randn(3,requires_grad=True)\n",
    "W2=torch.randn((3,10),requires_grad=True)\n",
    "b2=torch.randn(10,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define how computations will happen in the forward pass\n",
    "def net(W1,b1,W2,b2,X):\n",
    "    z1=torch.matmul(X.float(),W1)+b1\n",
    "    a1=torch.sigmoid(z1)\n",
    "    z2=torch.matmul(a1,W2)+b2\n",
    "    p=torch.softmax(z2,axis=1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce(p,y):\n",
    "    return -torch.log(p[range(y.shape[0]),y.long()]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.986879587173462\n",
      "Loss: 2.9846160411834717\n",
      "Loss: 2.98236346244812\n",
      "Loss: 2.9801225662231445\n",
      "Loss: 2.9778926372528076\n",
      "Loss: 2.975675106048584\n",
      "Loss: 2.973466634750366\n",
      "Loss: 2.97127103805542\n",
      "Loss: 2.969085454940796\n",
      "Loss: 2.9669106006622314\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(10):\n",
    "    p=net(W1,b1,W2,b2,X)\n",
    "    loss=ce(p,y)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W1-=lr*W1.grad\n",
    "        W2-=lr*W2.grad\n",
    "        b1-=lr*b1.grad\n",
    "        b2-=lr*b2.grad\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=np.array([[0.8,0.1,0.1],[0.7,0.2,0.1],[0.1,0.1,0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.1, 0.1],\n",
       "       [0.7, 0.2, 0.1],\n",
       "       [0.1, 0.1, 0.8]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0,0] ## 0,\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1,1] ## 1,\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2,2] # 2,\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 0.2, 0.8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[range(3),y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1=nn.Parameter(torch.randn((784,3),requires_grad=True))\n",
    "        self.b1=nn.Parameter(torch.randn(3,requires_grad=True))\n",
    "        self.W2=nn.Parameter(torch.randn((3,10),requires_grad=True))\n",
    "        self.b2=nn.Parameter(torch.randn(10,requires_grad=True))\n",
    "    def forward(self,X):\n",
    "        z1=torch.matmul(X.float(),self.W1)+self.b1\n",
    "        a1=torch.sigmoid(z1)\n",
    "        z2=torch.matmul(a1,self.W2)+self.b2\n",
    "        p=torch.softmax(z2,axis=1)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.9440, -2.2863,  0.5219],\n",
       "         [-0.1077,  0.3833,  2.2591],\n",
       "         [ 0.2427,  0.2368,  0.3286],\n",
       "         ...,\n",
       "         [-0.5358, -1.3426,  0.2377],\n",
       "         [ 0.1022,  0.8008, -0.2817],\n",
       "         [ 0.0049,  1.5412,  0.4890]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.6149, -1.5055,  0.4591], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.9947,  0.5324, -0.4181,  0.5281,  0.1517,  1.7104,  0.0648,  0.1053,\n",
       "          -0.9055, -0.1474],\n",
       "         [-0.1860,  0.4197, -0.5564,  1.6358, -1.2533, -0.4074, -0.3594, -1.3828,\n",
       "          -0.5344,  0.0820],\n",
       "         [ 0.2188, -3.7559, -1.1455, -1.0347,  2.7765,  0.2268, -0.3581,  0.9246,\n",
       "           0.5373, -0.8488]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.7605,  1.0399, -0.4413,  2.6787, -1.3123,  0.3906,  1.3570, -0.7819,\n",
       "         -2.3241,  0.1289], requires_grad=True)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in mod.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3.486020565032959\n",
      "Loss 3.4807889461517334\n",
      "Loss 3.4756033420562744\n",
      "Loss 3.4704623222351074\n",
      "Loss 3.465364933013916\n",
      "Loss 3.460310935974121\n",
      "Loss 3.4553003311157227\n",
      "Loss 3.4503302574157715\n",
      "Loss 3.445402145385742\n",
      "Loss 3.4405159950256348\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "for i in range(10):\n",
    "    p=mod(X)\n",
    "    loss=ce(p,y)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in mod.parameters():\n",
    "            p-=lr*p.grad\n",
    "        mod.zero_grad()\n",
    "    print(f\"Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer(input_dim=784, no_neurons=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
